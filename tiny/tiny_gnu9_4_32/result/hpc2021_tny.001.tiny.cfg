# Invocation command line:
# /lustre/SPEChpc/bin/harness/runhpc --rebuild --define model=omp --define ppn=2 --pmodel=OMP --config=aws_g3_gnu9.cfg --ranks=4 --threads=32 --reportable --output_format=all --output_root=/home/sam/g3_spec_results/tiny/tiny_gnu9_4_32 --iterations=2 tiny
# output_root used was "/home/sam/g3_spec_results/tiny/tiny_gnu9_4_32"
############################################################################
%ifndef %{label}         # IF label is not set use aws
%   define label aws
%endif

%ifndef %{model}         # IF model is not set use mpi
%   define model mpi
%endif

teeout = yes
makeflags=-j 10

# Tester Information
license_num     = ?
test_sponsor    = University of Bristol
tester          = University of Bristol

######################################################
# SUT Section
######################################################
#include: aws.inc
#  ----- Begin inclusion of 'aws.inc'
############################################################################
######################################################
# Example configuration information for a
# system under test (SUT) Section
######################################################
# General SUT info
system_vendor      = Mega Technology
system_name        = Big Compute
hw_avail           = Nov-2099
sw_avail           = Nov-2099

# Computation node info
# [Node_Description: Hardware]
node_compute_syslbl = TurboBlaster 5000
node_compute_order = 1
node_compute_count = 8
node_compute_purpose = compute
node_compute_hw_vendor = Mega Technology
node_compute_hw_model = Turblaster 5000
node_compute_hw_cpu_name = Turbo CPU
node_compute_hw_ncpuorder = 1 chips
node_compute_hw_nchips = 1
node_compute_hw_ncores = 64
node_compute_hw_ncoresperchip = 64
node_compute_hw_nthreadspercore = 1
node_compute_hw_cpu_char = Turbo up to 3.4 GHz
node_compute_hw_cpu_mhz = 2250
node_compute_hw_pcache = 32 KB I + 32 KB D on chip per core
node_compute_hw_scache = 512 KB I+D on chip per core
node_compute_hw_tcache000= 256 MB I+D on chip per chip
node_compute_hw_tcache001 = 16 MB shared / 4 cores
node_compute_hw_ocache = None
node_compute_hw_memory = 256 GB (8 x 32 GB 2Rx8 PC4-3200AA-R)
node_compute_hw_disk = 1 x 480 GB  SATA 2.5" SSD
node_compute_hw_other = None

#[Node_Description: Accelerator]
node_compute_hw_accel_model = Tesla V100-PCIE-16GB
node_compute_hw_accel_count = 4
node_compute_hw_accel_vendor= NVIDIA Corporation
node_compute_hw_accel_type  = GPU
node_compute_hw_accel_connect = PCIe 3.0 16x
node_compute_hw_accel_ecc    = Yes
node_compute_hw_accel_desc   = See Notes

#[Node_Description: Software]
node_compute_hw_adapter_fs_model = None
node_compute_hw_adapter_fs_count = 0
node_compute_hw_adapter_fs_slot_type = None
node_compute_hw_adapter_fs_data_rate = None
node_compute_hw_adapter_fs_ports_used = 0
node_compute_hw_adapter_fs_interconnect = None
node_compute_hw_adapter_fs_driver = None
node_compute_hw_adapter_fs_firmware = None
node_compute_sw_os000 = SUSE Linux Enterprise Linux Server 12
node_compute_sw_os001 = 4.12.14-94.41-default
node_compute_sw_localfile = xfs
node_compute_sw_sharedfile = None
node_compute_sw_state = Multi-user, run level 3
node_compute_sw_other = None

#[Fileserver]
node_fileserver_sw_state = Multi-User, run level 3
node_fileserver_sw_sharedfile = NFS
node_fileserver_sw_other = None
node_fileserver_sw_os = Red Hat Enterprise Linux Server release 7.6
node_fileserver_sw_localfile = None
node_fileserver_purpose = Fileserver
node_fileserver_order = 2
node_fileserver_syslbl = NFS
node_fileserver_hw_vendor = Big Storage Company
node_fileserver_hw_tcache = 39424 KB I+D on chip per chip
node_fileserver_hw_scache = 1 MB I+D on chip per core
node_fileserver_hw_pcache = 32 KB I + 32 KB D on chip per core
node_fileserver_hw_other = None
node_fileserver_hw_ocache = None
node_fileserver_hw_nthreadspercore = 1
node_fileserver_hw_ncpuorder = 1-2 chips
node_fileserver_hw_ncoresperchip = 28
node_fileserver_hw_ncores = 56
node_fileserver_hw_nchips = 2
node_fileserver_hw_model = BG650
node_fileserver_hw_memory = 768 GB (24 x 32 GB 2Rx4 PC4-2933Y-R)
node_fileserver_hw_disk = 1 x 1 TB 12 Gbps SAS 2.5" SSD (JBOD)
node_fileserver_hw_cpu_name = Intel Xeon Platinum 8280
node_fileserver_hw_cpu_mhz = 2700
node_fileserver_hw_cpu_char = None
node_fileserver_hw_adapter_fs_slot_type = PCI-Express 3.0 x16
node_fileserver_hw_adapter_fs_ports_used = 1
node_fileserver_hw_adapter_fs_interconnect = BG 5000 series
node_fileserver_hw_adapter_fs_firmware = 10.9.0.1.0
node_fileserver_hw_adapter_fs_driver = 10.9.1.0.15
node_fileserver_hw_adapter_fs_data_rate = 100 Gb/s
node_fileserver_hw_adapter_fs_count = 1
node_fileserver_count = 1

#[Interconnect]
interconnect_fs_syslbl = Big Interconnect Company
interconnect_fs_order = 0
interconnect_fs_purpose = MPI Traffic
interconnect_fs_hw_vendor = Big Interconnect Company
interconnect_fs_hw_model = BI 100 Series
interconnect_fs_hw_switch_fs_model000= BI 100 Series 48 Port 2
interconnect_fs_hw_switch_fs_model001 = PSU
interconnect_fs_hw_switch_fs_count = 1
interconnect_fs_hw_switch_fs_ports = 48
interconnect_fs_hw_topo = Mesh
interconnect_fs_hw_switch_fs_data_rate = 100 Gb/s
interconnect_fs_hw_switch_fs_firmware = 10.3.0.0.60

#######################################################################
# End of SUT section
# If this config file were to be applied to several SUTs, edits would
# be needed only ABOVE this point.
######################################################################
# ---- End inclusion of '/lustre/SPEChpc/config/aws.inc'

#[Software]
system_class = Homogenous Cluster
sw_compiler    = C/C++/Fortran: GCC Version 7.3.1
sw_mpi_library    = OpenMPI Version 4.1.1
#sw_mpi_other = None
#sw_other = None

node_compute_sw_os000 = Amazon Linux 2
node_compute_sw_os001 = Linux 5.10.112-108.499.amzn2.aarch64

#[Submit notes]
#notes_submit_000 = MPI startup command:
#notes_submit_005 =   srun command was used to start MPI jobs.

#######################################################################
# End of SUT section
######################################################################

######################################################################
# The header section of the config file.  Must appear
# before any instances of "section markers" (see below)
#
# ext = how the binaries you generated will be identified
# tune = specify "base" or "peak" or "all"
label         = %{label}_%{model}
tune          = base
output_format = text
use_submit_for_speed = 1

# Compiler Settings
default:
CC           = mpicc
CXX          = mpicxx
FC           = mpif90

# Compiler Version Flags
CC_VERSION_OPTION  = --version
CXX_VERSION_OPTION = --version
FC_VERSION_OPTION  = --version

# MPI options and binding environment, dependent upon Model being run
# Adjust to match your system

# MPI (CPU) settings
%if %{model} eq 'mpi'
   MPIRUN_OPTS = --map-by ppr:64:node
%endif

# MPI+OpenMP (CPU) settings
%if %{model} eq 'omp'
   MPIRUN_OPTS = --map-by ppr:%{ppn}:node:PE=$threads
%endif

MPIRUN_OPTS += --mca topo basic

submit = mpirun -np $ranks ${MPIRUN_OPTS} $command

#######################################################################
# Optimization

# Note that SPEC baseline rules require that all uses of a given compiler
# use the same flags in the same order. See the SPEChpc Run Rules
# for more details
#      http://www.spec.org/hpc2021/Docs/runrules.html
#
# OPTIMIZE    = flags applicable to all compilers
# FOPTIMIZE   = flags applicable to the Fortran compiler
# COPTIMIZE   = flags applicable to the C compiler
# CXXOPTIMIZE = flags applicable to the C++ compiler
#
# See your compiler manual for information on the flags available
# for your compiler

# Compiler flags applied to all models
default=base=default:
OPTIMIZE       = -Ofast
CPORTABILITY   = -lm
FPORTABILITY   = -fno-stack-arrays

# MPI (CPU) settings/flags
%if %{model} eq 'mpi'
   pmodel = MPI
%endif

# MPI+OpenMP (CPU) settings/flags
%if %{model} eq 'omp'
   pmodel = OMP
   OPTIMIZE += -fopenmp
%endif

# No peak flags set, so make peak use the same flags as base
default=peak=default:
basepeak=1

#######################################################################
# Portability
#######################################################################


